Time I have spent on this project:
Feb 5, 8-12, 15-19, 22-24, March 5, 16, April 9th, 16th, 20th, 21st, 26th, 27th, 28th, 29th, May 3rd, 4th

(Updated on 4/21/21)
Hours: ~129

Basic premise of what I have been working on for each day
5th; Think about what metrics I want to have in my algorithm. Which stats would BEST tell me which leads become customers

8th-10th; Read in the initial dataset. Look through it and see that it is AWFUL (big shock I know) do my best to clean it, 
    get rid of unnecessary data, or somehow change it to be more accurate to the column it is in (I don't believe there is a zip code
    called 'poasoejfpo','????', or 'hotmail.com')

11th-12th; Once the data is in a good enough spot to be worked with, start making the very first machine learning algorithm. Start with
    descision tree and go from there. Change some things in the data set or how I feed the data into the algorithm. Try an XKBoost
    next.

15th-17th; Feel confident that the model I made can accurately predict which leads turn into customers. Fail. Get stopped immedieatly
    by how starkly different the data is from the testing data. Feel bad about myself. 

18th-19th, 22-24th; Feel better about myself because my manager wants me to get some different numbers aobut leads and reps. Think
    about how I can use these new numbers to better my old algorithms. Start working on getting the 'Speed to Contact' numbers and
    the 'Time to Appointment' numbers. Use those for the main work project AND my new machine learning algorithms.  

Mar 5th; Spent an hour working through the dataset and tried to merge the speed to contact data with it

Mar 16th; I started working on my Network again. I tried changing the 
    epochs to be less or more, I changed the number of units in 
    a dense layer, I change the 'verbose' setting on my 'model.fit'
    and I cannot seem to get my accuracy above 0.501. I found
    a tutorial on structured data in the TensorFlow tutorials, so
    maybe that will help me get what I want. We will see.

March 17th; I cleaned up the datasets I have been working with so 
    it would not include any personal info. Now I am free to share
    it as I need to. 

April 9th, 16th, 19th,; As I looked through the analytics section of the Luminary app, I noticed I was not using 
    a bunch of numbers that could add great value to regressions and algorithims that I could make. So, I decided
    I wanted to add number of calls, number of leads claimed, appointments set, number of dials, talk time, and
    number of sales. As I went to get that information from the database, I have been unsuccessful. For whatever reason, 
    the JOINs of the tables lead_products, call_logs, appointments, and status_history. It is probably because 
    those tables are all massive... but still I am getting very spectific pieces of data from each table so it 
    shouldn't be too much to grab. I don't understand.

April 20th; I was finally able to get the data I wanted in a good format.
    It took some doing and some fancy wrangling of four different tables,
    but I got it. I set up the format to make it work with my simple 
    Nueral Network I made and it... didn't work? I say it like this
    because I got a result but, down to the 20th decimal place, it 
    is the exact same number that I got from the different set of
    data I used. So... really don't know what is going on here. I decided
    to completely change my network. I am starting to work with TensorFlow's
    tutorial on Structured Data along with classifying structured data
    with feature columns. We shall see what happens

April 21st; Finish up the final bits of the network. I tested it and    
    it is performing quite well! I am getting around an 81% accuracy. 
    Now I just need to figure out how to get a percentage chance of
    each lead becoming a customer. A softmax() function would probably
    do it, I just don't know how to implement it. 

April 26th; Started work on my blog. I installed distill. I made my 
    first .Rmd blog post. I don't really know where to go from there.

April 27th; I had Sister Larson help me get my blog ready to deploy.
    She helped me make a new repo in GitHub and she explained how 
    to set it up so that GitHub will host my blog. 
    Along with that I put more work into my network. I was able to
    finally get an output! I just needed to use the function 
    .predict() ... Go Figure. Actually not quite THAT simple.
    I noticed in the TensorFlow website/tutorial I am working with
    I am using model classes and there was a '.predict_classes()' 
    function. So I tried that and it gave me a column of 1s and 0s. 
    I am... farily confident that is correct. I mean, I checked it
    against the dataset I used and... it looked right to me. I also 
    looked up how to get a probability from the model and it looks
    like I need to use Softmax. I will look into that more... later.

April 28th; I did some reasearch about show feature importance for 
    my Tensor Flow Neural Network. As I was doing my reasearch, I
    saw a lot of packages that were able to explain image classification
    networks but not classification of traditional data points. So then
    it occured to me.... am I bringing a bazooka to a street fight...
    Am I COMPLETELY over complicating this project when I could just
    use a tree classifier or a random forest...? Something to think about.

April 29th; For whatever reason, I lost my blog in Rstudio. I don't know 
    how it happened, but I needed to redo it and reinitialize the entire
    repo. So I was able to do that and I got it up and running again and NOW 
    I know how to properly build the website and blog so that it will link 
    to my other posts. I also talked to Brother Palmer about my progress
    over all. I described my thought about my Neural Network being overkill
    and he more or less agreed with me. He also turned me onto this 
    cool software called JMP. It is a way to easily visualize descision
    trees. Now I just need to work on explaining how my descision tree
    does what it does. 
    I... got it... I freaking got it. I looked up another logistic
    classifier model that used a random forest. I was looking 
    through the API and I found that it had a predict_proba()
    function. I ran that function and it... gave me a probability.
    How about that! Now! I just need to figure out how to link the
    lead_id to the predicted probability. 
    We are almost there boys and girls. We are almost there.

May 3nd - May the 4th be with you ;) I played around with getting
    the model to do what I wanted and tried to figure out how I could
    describe what it does and what reps can do to up thier chances 
    of keeping a lead. I also started work on getting new data and 
    running that through my Ab Model. I have the new data ready and
    I just need to merge it into one data table.